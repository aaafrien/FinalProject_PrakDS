df <- data.frame(word = names(words),freq=words)
tweets_words <-  data1 %>%
select(text) %>%
unnest_tokens(word, text)
library(tm) #data cleaning (corpus)
library(twitteR) #akses twitter APIs
library(rtweet) #collect and organize twitter data
library(shiny) #shiny
library(syuzhet) #baca fungsi get_nrc
library(wordcloud) #wordcloud
library(vroom) #load dataset
library(here) #menyimpan dataset
library(dplyr) #manipulasi data frame
library(ggplot2) #visualisasi data (barplot, grafik)
library(RColorBrewer) #pengaturan warna
library(RTextTools) #buat naive bayes
library(tidytext)
library(tm) #data cleaning (corpus)
library(twitteR) #akses twitter APIs
library(rtweet) #collect and organize twitter data
library(shiny) #shiny
library(syuzhet) #baca fungsi get_nrc
library(wordcloud) #wordcloud
library(vroom) #load dataset
library(here) #menyimpan dataset
library(dplyr) #manipulasi data frame
library(ggplot2) #visualisasi data (barplot, grafik)
library(RColorBrewer) #pengaturan warna
library(RTextTools) #buat naive bayes
library(tidytext)
library(wordcloud2)
data1 <- read.csv('wadasLabel.csv')
text <- data1$text
docs <- Corpus(VectorSource(text))
docs <- tm_map(docs, removeWords,"yang")
docs <- tm_map(docs, removeWords,"oleh")
docs <- tm_map(docs, removeWords,"tih")
dtm <- TermDocumentMatrix(docs)
matrix <- as.matrix(dtm)
words <- sort(rowSums(matrix),decreasing=TRUE)
df <- data.frame(word = names(words),freq=words)
tweets_words <-  data1 %>%
select(text) %>%
unnest_tokens(word, text)
words <- tweets_words %>% dplyr::count(word, sort=TRUE)
p <- wordcloud2(data=df, size=1.6, color='random-dark')
#tempat data akan dianalisis dan diproses, hasilnya ditampilkan/diplotkan pada bagian mainpanel() ui
server <- function(input, output) {
#output Data
output$result <-renderPrint({
conf.matNB
})
wadasLabel <- read.csv('wadasLabel.csv')
output$tbl = DT::renderDataTable({
DT::datatable(wadasLabel, options = list(lengthChange = FALSE)) #data ditampilkan dalam beberapa halaman
})
#barplot
output$scatterplot <- renderPlot({
barplot(colSums(s), col=rainbow(10), ylab='count',main='Sentiment Analysis')
}, height = 400)
#freq Plot
output$freqplot <- renderPlot({
barplot(d[1:20,]$freq, las = 2, names.arg = d[1:20,]$word, col=rainbow(5),
main = "Kata Paling Sering Muncul", ylab = "Frekuensi")
}, height = 400)
#wordcloud
output$Wordcloud2 <- renderWordcloud2({
p
})
}
shinyApp(ui = ui, server = server)
library(wordcloud2)
data1 <- read.csv('wadasLabel.csv')
text <- data1$text
docs <- Corpus(VectorSource(text))
docs <- tm_map(docs, removeWords,"yang")
docs <- tm_map(docs, removeWords,"oleh")
docs <- tm_map(docs, removeWords,"tih")
dtm <- TermDocumentMatrix(docs)
matrix <- as.matrix(dtm)
words <- sort(rowSums(matrix),decreasing=TRUE)
df <- data.frame(word = names(words),freq=words)
tweets_words <-  data1 %>%
select(text) %>%
unnest_tokens(word, text)
words <- tweets_words %>% dplyr::count(word, sort=TRUE)
p <- wordcloud2(data=df, size=1.6, color='random-dark')
p
tweets = searchTwitter('Wadas', n = 2000, retryOnRateLimit = 10e5, lang = "id") #retryOnRateLimit untuk looping
text <- do.call("rbind", lapply(tweets, as.data.frame))
write.csv(text, file = 'wadas.rds')
set <- read.csv('wadas.rds')
d = twListToDF(tweets)
corpus <- d$text
corpus <- Corpus(VectorSource(corpus))
#hapus URL
removeURL <- function(x) gsub("https\\S*", "", x)
twtclean <- tm_map(corpus, removeURL)
#hapus New Line
removeNL <- function(y) gsub("\n", "", y)
twtclean <- tm_map(twtclean, removeNL)
#hapus koma
replacecomma <- function(y) gsub(",", "", y)
twtclean <- tm_map(twtclean, replacecomma)
#hapus retweet
removeRT <- function(y) gsub("^RT:?", "", y)
twtclean <- tm_map(twtclean, removeRT)
#hapus titik koma
removetitikkoma <- function(y) gsub(";", " ", y)
twtclean <- tm_map(twtclean, removetitikkoma)
#hapus titik dua
removetitik2 <- function(y) gsub(":", "", y)
twtclean <- tm_map(twtclean, removetitik2)
#hapus titik tiga
removetitik3 <- function(y) gsub("p.", "", y)
twtclean <- tm_map(twtclean, removetitik3)
#hapus &amp
removeamp <- function(y) gsub("&amp;", "", y)
twtclean <- tm_map(twtclean, removeamp)
#hapus mention
removeUN <- function(z) gsub("@[A-Za-z0-9]+", "", z)
twtclean <- tm_map(twtclean, removeUN)
#hapus space dll
remove.all <- function(xy) gsub("[^[:alpha:][:space:]]*", "", xy)
twtclean <- tm_map(twtclean,stripWhitespace)
inspect(twtclean[1:10])
twtclean <- tm_map(twtclean,remove.all)
twtclean <- tm_map(twtclean, removePunctuation) #tanda baca
twtclean <- tm_map(twtclean, tolower) #mengubah huruf kecil
textClean <- data.frame(text=unlist(sapply(twtclean, paste, collapse = " ")), stringsAsFactors=F)
View(textClean)
write.csv(textClean,'wadasClean.csv')
wadasClean <- read.csv("wadasClean.csv", header=T)
#scoring
kata.positif <- scan("positive-words.txt",what="character",comment.char=";")
kata.negatif <- scan("negative-words.txt",what="character",comment.char=";")
score.sentiment = function(sentence, positif, negatif, .progress='none') {
require(plyr)
require(stringr)
scores = laply(sentence, function(kalimat, positif, negatif) {
kalimat = gsub('[[:punct:]]', '', kalimat)
kalimat = gsub('[[:cntrl:]]', '', kalimat)
kalimat = gsub('\\d+', '', kalimat)
kalimat = tolower(kalimat)
list.kata = str_split(kalimat, '\\s+')
kata2 = unlist(list.kata)
positif.matches = match(kata2, kata.positif)
negatif.matches = match(kata2, kata.negatif)
positif.matches = !is.na(positif.matches)
negatif.matches = !is.na(negatif.matches)
score = sum(positif.matches) - (sum(negatif.matches))
return(score)
},
kata.positif, kata.negatif, .progress=.progress)
scores.df = data.frame(score=scores, text=sentence)
return(scores.df)
}
hasil = score.sentiment(wadasClean$text, kata.positif, kata.negatif)
#convert score to sentiment
hasil$klasifikasi <- ifelse(hasil$score<0, "Negatif", ifelse(hasil$score==0,"Netral","Positif"))
hasil$klasifikasi
View(hasil)
data <- hasil[c(3,1,2)] #ubah urutan kolom
View(data)
write.csv(data, file = "wadasLabel.csv")
library(e1071) #library yang terdapat sebuah algoritma naivebayes
library(caret) #library yang terdapat sebuah algoritma naivebayes
library(syuzhet) #library yang terdapat sebuah algoritma naivebayes
#digunakan untuk membaca file csv yang sudah di cleaning data
wadas_dataset <- read.csv("wadasClean.csv", stringsAsFactors = FALSE)
#digunakan untuk mengeset variabel kolom text menjadi char
review <- as.character(wadas_dataset$text)
#memanggil sentimen dictionary untuk menghitung presentasi dari beberapa emotion dan mengubahnya ke dalam text file
get_nrc_sentiment('happy')
get_nrc_sentiment('excitement')
s <- get_nrc_sentiment(review)
review_combine <- cbind(wadas_dataset$text,s)
par(mar=rep(3,4))
barplot(colSums(s), col=rainbow(10), ylab='count',main='Sentiment Analysis')
require(corpus)
data.frame <- read.csv("wadasLabel.csv", stringsAsFactors = F)
data.frame$klasifikasi <- factor(data.frame$klasifikasi)
glimpse(data.frame)
set.seed(20)
data.frame <- data.frame[sample(nrow(data.frame)),]
data.frame <- data.frame[sample(nrow(data.frame)),]
glimpse(data.frame)
corpus <- Corpus(VectorSource(data.frame$text))
corpus
inspect(corpus[1:10])
#membersihkan data yang tidak diperlukan
corpus.clean <- corpus %>%
tm_map(content_transformer(tolower)) %>% #mengubah string huruf besar menjadi string huruf kecil
tm_map(removePunctuation) %>% #menghapus tanda baca
tm_map(removeNumbers) %>% #menghapus nomor
tm_map(removeWords, stopwords(kind="en")) %>% #menghapus stopwords
tm_map(stripWhitespace)
dtm <- DocumentTermMatrix(corpus.clean)
inspect(dtm[1:10,1:20])
df.train<-data.frame[1:340,]
df.test<-data.frame[341:680,]
dtm.train<-dtm[1:340,]
dtm.test<-dtm[341:680,]
corpus.clean.train<-corpus.clean[1:340]
corpus.clean.test<-corpus.clean[341:680]
dim(dtm.train) #set the dimension of an object
fivefreq <- findFreqTerms(dtm.train, 5)
length(fivefreq)
dtm.train.nb <- DocumentTermMatrix(corpus.clean.train, control = list(dictionary=fivefreq))
dim(dtm.train.nb)
dtm.test.nb <- DocumentTermMatrix(corpus.clean.test, control = list(dictionary=fivefreq))
dim(dtm.test.nb)
#boolean naive bayes
convert_count <- function(x){
y <- ifelse(x>0,1,0)
y <- factor(y,levels=c(0,1),labels=c("no","yes"))
y
}
#naive bayes model
trainNB<-apply(dtm.train.nb,2,convert_count)
testNB<-apply(dtm.test.nb,2,convert_count)
#training data
classifier <- naiveBayes(trainNB, df.train$klasifikasi, laplace = 1)
#use NB classifier to make predictions on the test set
pred <- predict(classifier, testNB)
#membuat tabel kebenaran dengan mentabulasi predicted class labels with the actual class labels
NB_table=table("Prediction"= pred, "Actual" = df.test$klasifikasi)
NB_table
#confussion matrix
conf.matNB <- confusionMatrix(pred, df.test$klasifikasi)
conf.matNB
library(wordcloud2)
data1 <- read.csv('wadasLabel.csv')
text <- data1$text
docs <- Corpus(VectorSource(text))
docs <- tm_map(docs, removeWords,"yang")
docs <- tm_map(docs, removeWords,"oleh")
docs <- tm_map(docs, removeWords,"tih")
dtm <- TermDocumentMatrix(docs)
matrix <- as.matrix(dtm)
words <- sort(rowSums(matrix),decreasing=TRUE)
df <- data.frame(word = names(words),freq=words)
tweets_words <-  data1 %>%
select(text) %>%
unnest_tokens(word, text)
words <- tweets_words %>% dplyr::count(word, sort=TRUE)
p <- wordcloud2(data=df, size=1.6, color='random-dark')
p
data1 = read.csv("wadasLabel.csv")
corpus = Corpus(VectorSource(data1$text))
corpus <- tm_map(corpus, removeWords,"yang")
corpus <- tm_map(corpus, removeWords,"oleh")
corpus <- tm_map(corpus, removeWords,"tih")
dtm <- TermDocumentMatrix(corpus)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
barplot(d[1:20,]$freq, las = 2, names.arg = d[1:20,]$word, col=rainbow(5),
main = "Kata Paling Sering Muncul", ylab = "Frekuensi")
library(wordcloud2)
data1 <- read.csv('wadasLabel.csv')
text <- data1$text
docs <- Corpus(VectorSource(text))
docs <- tm_map(docs, removeWords,"yang")
docs <- tm_map(docs, removeWords,"oleh")
docs <- tm_map(docs, removeWords,"tih")
dtm <- TermDocumentMatrix(docs)
matrix <- as.matrix(dtm)
words <- sort(rowSums(matrix),decreasing=TRUE)
df <- data.frame(word = names(words),freq=words)
tweets_words <-  data1 %>%
select(text) %>%
unnest_tokens(word, text)
words <- tweets_words %>% dplyr::count(word, sort=TRUE)
p <- wordcloud2(data=df, size=1.6, color='random-dark')
p
library(wordcloud2)
data1 <- read.csv('wadasLabel.csv')
text <- data1$text
docs <- Corpus(VectorSource(text))
docs <- tm_map(docs, removeWords,"yang")
docs <- tm_map(docs, removeWords,"oleh")
docs <- tm_map(docs, removeWords,"tih")
dtm <- TermDocumentMatrix(docs)
matrix <- as.matrix(dtm)
words <- sort(rowSums(matrix),decreasing=TRUE)
df <- data.frame(word = names(words),freq=words)
tweets_words <-  data1 %>%
select(text) %>%
unnest_tokens(word, text)
words <- tweets_words %>% dplyr::count(word, sort=TRUE)
p <- wordcloud2(data=df, size=1.6, color='random-dark')
p
library(wordcloud2)
data1 <- read.csv('wadasLabel.csv')
text <- data1$text
docs <- Corpus(VectorSource(text))
docs <- tm_map(docs, removeWords,"yang")
docs <- tm_map(docs, removeWords,"oleh")
docs <- tm_map(docs, removeWords,"tih")
dtm <- TermDocumentMatrix(docs)
matrix <- as.matrix(dtm)
words <- sort(rowSums(matrix),decreasing=TRUE)
df <- data.frame(word = names(words),freq=words)
tweets_words <-  data1 %>%
select(text) %>%
unnest_tokens(word, text)
words <- tweets_words %>% dplyr::count(word, sort=TRUE)
p <- wordcloud2(data=df, size=1.6, color='random-dark')
p
library(wordcloud2)
data1 <- read.csv('wadasLabel.csv')
text <- data1$text
docs <- Corpus(VectorSource(text))
docs <- tm_map(docs, removeWords,"yang")
docs <- tm_map(docs, removeWords,"oleh")
docs <- tm_map(docs, removeWords,"tih")
dtm <- TermDocumentMatrix(docs)
matrix <- as.matrix(dtm)
words <- sort(rowSums(matrix),decreasing=TRUE)
df <- data.frame(word = names(words),freq=words)
tweets_words <-  data1 %>%
select(text) %>%
unnest_tokens(word, text)
words <- tweets_words %>% dplyr::count(word, sort=TRUE)
p <- wordcloud2(data=df, size=1.6, color='random-dark')
p
library(wordcloud2)
data1 <- read.csv('wadasLabel.csv')
text <- data1$text
docs <- Corpus(VectorSource(text))
docs <- tm_map(docs, removeWords,"yang")
docs <- tm_map(docs, removeWords,"oleh")
docs <- tm_map(docs, removeWords,"tih")
dtm <- TermDocumentMatrix(docs)
matrix <- as.matrix(dtm)
words <- sort(rowSums(matrix),decreasing=TRUE)
df <- data.frame(word = names(words),freq=words)
tweets_words <-  data1 %>%
select(text) %>%
unnest_tokens(word, text)
words <- tweets_words %>% dplyr::count(word, sort=TRUE)
p <- wordcloud2(data=df, size=1.6, color='random-dark')
p
library(wordcloud2)
data1 <- read.csv('wadasLabel.csv')
text <- data1$text
docs <- Corpus(VectorSource(text))
docs <- tm_map(docs, removeWords,"yang")
docs <- tm_map(docs, removeWords,"oleh")
docs <- tm_map(docs, removeWords,"tih")
dtm <- TermDocumentMatrix(docs)
matrix <- as.matrix(dtm)
words <- sort(rowSums(matrix),decreasing=TRUE)
df <- data.frame(word = names(words),freq=words)
tweets_words <-  data1 %>%
select(text) %>%
unnest_tokens(word, text)
words <- tweets_words %>% dplyr::count(word, sort=TRUE)
p <- wordcloud2(data=df, size=1.6, color='random-dark')
p
library(wordcloud2)
data1 <- read.csv('wadasLabel.csv')
text <- data1$text
docs <- Corpus(VectorSource(text))
docs <- tm_map(docs, removeWords,"yang")
docs <- tm_map(docs, removeWords,"oleh")
docs <- tm_map(docs, removeWords,"tih")
dtm <- TermDocumentMatrix(docs)
matrix <- as.matrix(dtm)
words <- sort(rowSums(matrix),decreasing=TRUE)
df <- data.frame(word = names(words),freq=words)
tweets_words <-  data1 %>%
select(text) %>%
unnest_tokens(word, text)
words <- tweets_words %>% dplyr::count(word, sort=TRUE)
p <- wordcloud2(data=df, size=1.6, color='random-dark')
p
library(wordcloud2)
data1 <- read.csv('wadasLabel.csv')
text <- data1$text
docs <- Corpus(VectorSource(text))
docs <- tm_map(docs, removeWords,"yang")
docs <- tm_map(docs, removeWords,"oleh")
docs <- tm_map(docs, removeWords,"tih")
dtm <- TermDocumentMatrix(docs)
matrix <- as.matrix(dtm)
words <- sort(rowSums(matrix),decreasing=TRUE)
df <- data.frame(word = names(words),freq=words)
tweets_words <-  data1 %>%
select(text) %>%
unnest_tokens(word, text)
words <- tweets_words %>% dplyr::count(word, sort=TRUE)
p <- wordcloud2(data=df, size=1.6, color='random-dark')
p
library(wordcloud2)
data1 <- read.csv('wadasLabel.csv')
text <- data1$text
docs <- Corpus(VectorSource(text))
docs <- tm_map(docs, removeWords,"yang")
docs <- tm_map(docs, removeWords,"oleh")
docs <- tm_map(docs, removeWords,"tih")
docs <- tm_map(docs, removeWords,"rambutnya")
dtm <- TermDocumentMatrix(docs)
matrix <- as.matrix(dtm)
words <- sort(rowSums(matrix),decreasing=TRUE)
df <- data.frame(word = names(words),freq=words)
tweets_words <-  data1 %>%
select(text) %>%
unnest_tokens(word, text)
words <- tweets_words %>% dplyr::count(word, sort=TRUE)
p <- wordcloud2(data=df, size=1.6, color='random-dark')
p
data1 = read.csv("wadasLabel.csv")
corpus = Corpus(VectorSource(data1$text))
corpus <- tm_map(corpus, removeWords,"yang")
corpus <- tm_map(corpus, removeWords,"oleh")
corpus <- tm_map(corpus, removeWords,"tih")
dtm <- TermDocumentMatrix(corpus)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
barplot(d[1:20,]$freq, las = 2, names.arg = d[1:20,]$word, col=rainbow(5),
main = "Kata Paling Sering Muncul", ylab = "Frekuensi")
#shiny
#membuka file csv
twitter <- read.csv(file="wadasClean.csv", header=TRUE)
#membuka text file pada data frame twitter
tweet <- twitter$text
#mengatur tampilan web
ui <- fluidPage(
titlePanel("Penggunaan Kata Wadas Pada Twitter"), #judul
mainPanel( #tab
#plot output : untuk scatterplot
tabsetPanel(type = "tabs",
tabPanel("Term Document Matrix and Statistic", verbatimTextOutput("result")),
tabPanel("Histogram", plotOutput("scatterplot")), #tab berupa histogram
tabPanel("Frequency", plotOutput("freqplot")), #tab berupa frequency
tabPanel("Data Twitter", DT::dataTableOutput('tbl')), #tab berupa data cleaning twitter & skoring
tabPanel("Wordcloud", wordcloud2Output("Wordcloud2")) #tab berupa worldcloud
)
)
)
#tempat data akan dianalisis dan diproses, hasilnya ditampilkan/diplotkan pada bagian mainpanel() ui
server <- function(input, output) {
#output Data
output$result <-renderPrint({
conf.matNB
})
wadasLabel <- read.csv('wadasLabel.csv')
output$tbl = DT::renderDataTable({
DT::datatable(wadasLabel, options = list(lengthChange = FALSE)) #data ditampilkan dalam beberapa halaman
})
#barplot
output$scatterplot <- renderPlot({
barplot(colSums(s), col=rainbow(10), ylab='count',main='Sentiment Analysis')
}, height = 400)
#freq Plot
output$freqplot <- renderPlot({
barplot(d[1:20,]$freq, las = 2, names.arg = d[1:20,]$word, col=rainbow(5),
main = "Kata Paling Sering Muncul", ylab = "Frekuensi")
}, height = 400)
#wordcloud
output$Wordcloud2 <- renderWordcloud2({
p
})
}
shinyApp(ui = ui, server = server)
shinyApp(ui = ui, server = server)
library(wordcloud2)
data1 <- read.csv('wadasLabel.csv')
text <- data1$text
docs <- Corpus(VectorSource(text))
docs <- tm_map(docs, removeWords,"yang")
docs <- tm_map(docs, removeWords,"oleh")
docs <- tm_map(docs, removeWords,"tih")
docs <- tm_map(docs, removeWords,"rambutnya")
dtm <- TermDocumentMatrix(docs)
matrix <- as.matrix(dtm)
words <- sort(rowSums(matrix),decreasing=TRUE)
df <- data.frame(word = names(words),freq=words)
tweets_words <-  data1 %>%
select(text) %>%
unnest_tokens(word, text)
words <- tweets_words %>% dplyr::count(word, sort=TRUE)
p <- wordcloud2(data=df, size=2, color='random-dark')
p
#tempat data akan dianalisis dan diproses, hasilnya ditampilkan/diplotkan pada bagian mainpanel() ui
server <- function(input, output) {
#output Data
output$result <-renderPrint({
conf.matNB
})
wadasLabel <- read.csv('wadasLabel.csv')
output$tbl = DT::renderDataTable({
DT::datatable(wadasLabel, options = list(lengthChange = FALSE)) #data ditampilkan dalam beberapa halaman
})
#barplot
output$scatterplot <- renderPlot({
barplot(colSums(s), col=rainbow(10), ylab='count',main='Sentiment Analysis')
}, height = 400)
#freq Plot
output$freqplot <- renderPlot({
barplot(d[1:20,]$freq, las = 2, names.arg = d[1:20,]$word, col=rainbow(5),
main = "Kata Paling Sering Muncul", ylab = "Frekuensi")
}, height = 400)
#wordcloud
output$Wordcloud2 <- renderWordcloud2({
p
})
}
shinyApp(ui = ui, server = server)
